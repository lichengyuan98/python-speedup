{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "仅仅使用ufunc并不能够进行复杂的并行操作，因此需要使用CUDA Kernel函数进行计算的并行化。\n",
    "在这里，需要对CUDA C并行计算方法有基本的认识。\n",
    "计算中需要将任务空间分配给多个`grid`共同完成。一个`grid`由多个`block`组成，每个`block`中由多个`thread`组成。\n",
    "`thread`运行在一个核心上，`block`运行在Streaming Multiprocessor(SM)上，`grid`运行在整个GPU卡上\n",
    "这种任务分配方式遵循以下规则：\n",
    "+ `block`中的`thread`数量应该是32的倍数，且数量位于**128~512**之间\n",
    "+ `grid`中的`block`数量应确保尽可能地利用整个GPU。在启动计算阶段，`block`的数量是GPU上**多处理器数量的2-4倍**，通常在**20~100**之间。\n",
    "+ CUDA内核的启动开销确实取决于块的数量，所以当输入规模非常大时，最好不要启动线程数量等于输入元素数量的网格。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    block_size = cuda.blockDim.x  # 每个block中的thread数量\n",
    "    grid_size = cuda.gridDim.x  # 每个grid中的block数量\n",
    "\n",
    "    tx = cuda.threadIdx.x  # block中thread的索引，编号从0 ~ blockDim.x-1\n",
    "    ty = cuda.blockIdx.x  # gird中block的索引，编号从0 ~ gridDim.x-1\n",
    "\n",
    "    start = tx + ty * block_size  # thread_ID + block_ID * block_size\n",
    "    stride = block_size * grid_size  # 整个grid有多少thread\n",
    "\n",
    "    # assuming x and y inputs are same length\n",
    "    for i in range(start, x.shape[0], stride):\n",
    "        out[i] = x[i] + y[i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.88 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "n = 1_000_000\n",
    "x = cp.arange(n).astype(cp.float32)\n",
    "y = 2 * x\n",
    "out = cp.empty_like(x)\n",
    "\n",
    "threads_per_block = 128\n",
    "blocks_per_grid = 30\n",
    "\n",
    "% timeit -r 1 -n 5 add_kernel[blocks_per_grid, threads_per_block](x, y, out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "上面两个cell可以写得更简洁一些，如下所示"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    start = cuda.grid(1)  # 一维array空间的thread起点\n",
    "    stride = cuda.gridsize(1)  # 每个grid包含的thread数量\n",
    "\n",
    "    # 下面循环的含义是\n",
    "    # i代表了每个grid计算任务的thread ID\n",
    "    # 给定了每个grid起始的thread ID后，该grid中每个thread做着循环体中相同的工作\n",
    "    for i in range(start, x.shape[0], stride):\n",
    "        out[i] = x[i] + y[i]\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_kernel_thread(x, y, out):\n",
    "    threadID = cuda.grid(1)  # 对于一维array，grid中thread的编号\n",
    "    if threadID < x.shape[0]:  # 对于在1d array长度范围内的数组的元素进行操作\n",
    "        out[threadID] = x[threadID] + y[threadID]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "1.05 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "n = 10_000_000\n",
    "\n",
    "x = cp.arange(n).astype(cp.float32)\n",
    "y = x * 2\n",
    "out = cp.empty_like(x)\n",
    "\n",
    "threads_per_block = 128\n",
    "blocks_per_grid = 30\n",
    "\n",
    "\n",
    "% timeit -r 1 -n 1 add_kernel[blocks_per_grid, threads_per_block](x, y, out)\n",
    "# 由于host在调用CUDA kernel函数后，cpu和gpu便异步计算.\n",
    "# 调用`cuda.synchronize()`便能使host等待kernel计算完成后再进行下步计算。\n",
    "# 因此一般在调用kernel函数的下一行加入此代码。\n",
    "cuda.synchronize()\n",
    "\n",
    "% timeit -r 1 -n 1 add_kernel_thread[blocks_per_grid, threads_per_block](x, y, out)\n",
    "cuda.synchronize()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来测试一个三维array的相加操作"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_kernel_3d(x, y, out):\n",
    "    i, j, k = cuda.grid(3)\n",
    "    size = x.shape\n",
    "    if i < size[0] and j < size[1] and k < size[2]:\n",
    "        out[i, j, k] = x[i, j, k] + y[i, j, k]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.38 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "n = 500\n",
    "x = cp.arange(n ** 3).reshape(n, n, n)\n",
    "y = x * 2\n",
    "out = cp.empty_like(x)\n",
    "\n",
    "threads_per_block = (8, 8, 8)\n",
    "blocks_per_grid = (4, 4, 4)\n",
    "\n",
    "%timeit -r 1 -n 10 add_kernel_3d[threads_per_block, blocks_per_grid](x, y, out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "cp.get_default_pinned_memory_pool().free_all_blocks()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}