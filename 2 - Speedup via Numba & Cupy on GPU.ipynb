{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "被操作的对象必须是一个张量，因此需要把标量转化为一个0维张量，形参是对实参地址的指针。\n",
    "这种不显式分配GPU内存的方法，会由于数据在CPU和GPU之间来回搬运造成计算能力的损耗\n",
    "在下面这个代码块中，numba自动完成了以下事情：\n",
    "+ 编译了一个CUDA内核，在所有的输入元素上并行地执行ufunc操作。\n",
    "+ 为输入和输出分配GPU内存。\n",
    "+ 将输入数据复制到GPU上。\n",
    "+ 根据输入的大小，以正确的内核尺寸执行CUDA内核。\n",
    "+ 将结果从GPU复制到CPU。\n",
    "+ 将结果以NumPy数组的形式返回到主机上。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def check_prime_gpu_kernel(num, result):\n",
    "    for i in range(2, num // 2 + 1):\n",
    "        if (num % i) == 0:\n",
    "            result[0] = 0\n",
    "    else:\n",
    "        result[0] = num\n",
    "\n",
    "\n",
    "def find_all_primes_cpu_and_gpu(upper):\n",
    "    all_prime_numbers = []\n",
    "    for num in range(2, upper):\n",
    "        result = np.zeros((1), np.int32)\n",
    "        # 在下步中完成数据cpu->gpu、GPU计算、数据gpu->cpu。此外[1, 1]有着特殊含义\n",
    "        check_prime_gpu_kernel[1, 1](num, result)\n",
    "\n",
    "        if result[0] > 0:\n",
    "            all_prime_numbers.append(num)\n",
    "    return all_prime_numbers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "[2, 3, 4, 5, 6, 7, 8, 9]"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all_primes_cpu_and_gpu(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "利用ufunc进行计算加速。下面代码首先使用cpu并行进行加速"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [],
   "source": [
    "# 定义一个ufunc\n",
    "import math\n",
    "import numba\n",
    "from numba import vectorize\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True,\n",
    "           fastmath=True,\n",
    "           nogil=True)\n",
    "def is_prime(num):\n",
    "    flag = True\n",
    "    for i in range(2, math.floor(math.sqrt(num)) + 1):\n",
    "        if (num % i) == 0:\n",
    "            flag = False\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\n",
    "@vectorize(['int32(int32)'], target='cpu')\n",
    "def check_prime(num):\n",
    "    flag = 0\n",
    "    if is_prime(num):\n",
    "        flag = 1\n",
    "    return flag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.15 s ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = 10_000_000\n",
    "test_np_array = np.arange(2, n)\n",
    "%timeit -r 1 -n 5 out = check_prime(test_np_array)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面代码再使用在gpu上的ufunc进行加速，加速的方法包含有：\n",
    "+ `@numba.cuda.jit(device=True)`指定cuda上操作\n",
    "+ `@vectorize([<type_expr>], target='cuda')`指定ufunc在cuda上操作\n",
    "+ ufunc操作的对象由`cupy`已经创建在cuda上了\n",
    "\n",
    "值得注意的是，利用cuda计算的函数内部只能包含以下操作：\n",
    "+ `if`/`elif`/`else`\n",
    "+ `while` and `for` loops\n",
    "+ Basic math operators\n",
    "+ Selected functions from the `math` and `cmath` modules\n",
    "+ Tuples\n",
    "\n",
    "需要明确的是，在cuda上进行的操作最好的方法是先开辟一块内存空间后（开辟空间的计算消耗较大），直接在已开辟的空间上进行数值运算。相较于cpu上的计算，已经达到20~30倍的加速了。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [],
   "source": [
    "import math\n",
    "import numba\n",
    "from numba import vectorize\n",
    "\n",
    "\n",
    "@numba.cuda.jit(device=True)\n",
    "def is_prime(num):\n",
    "    flag = True\n",
    "    for i in range(2, math.floor(math.sqrt(num)) + 1):\n",
    "        if (num % i) == 0:\n",
    "            flag = False\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "\n",
    "@vectorize(['int32(int32)'], target='cuda')\n",
    "def check_prime(num):\n",
    "    flag = 0\n",
    "    if is_prime(num):\n",
    "        flag = 1\n",
    "    return flag"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "n = 10_000_000\n",
    "test_cp_array = cp.arange(2, n)\n",
    "%timeit -r 1 -n 5 out = check_prime(test_cp_array)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来考虑多种在gpu上分配内存的方法。\n",
    "显然，利用cupy直接在gpu上分配的速度最快。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n",
      "210 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n",
      "42.3 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n",
      "982 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n",
      "98.1 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import numba\n",
    "n = 100_000_000\n",
    "%timeit -r 1 -n 5 a_device = numba.cuda.to_device(np.arange(n)) # cpu -> gpu via numba\n",
    "%timeit -r 1 -n 5 b_device = cp.asarray(np.arange(n)) # cpu -> gpu via cupy\n",
    "%timeit -r 1 -n 5 c_empty_device = numba.cuda.device_array(n) # like np.empty() via numba\n",
    "%timeit -r 1 -n 5 d_empty_device = cp.empty(n) # like np.empty() via cupy\n",
    "%timeit -r 1 -n 5 e_device = cp.arange(n) # allocate directly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [
    {
     "data": {
      "text/plain": "44400640"
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mempool = cp.get_default_memory_pool()\n",
    "pinned_mempool = cp.get_default_pinned_memory_pool()\n",
    "mempool.total_bytes()\n",
    "mempool.free_all_blocks()\n",
    "pinned_mempool.free_all_blocks()\n",
    "mempool.total_bytes()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}